{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3212303b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/xuenhui/anaconda3/envs/patch_base-py362/lib/python36.zip',\n",
       " '/home/xuenhui/anaconda3/envs/patch_base-py362/lib/python3.6',\n",
       " '/home/xuenhui/anaconda3/envs/patch_base-py362/lib/python3.6/lib-dynload',\n",
       " '',\n",
       " '/home/xuenhui/anaconda3/envs/patch_base-py362/lib/python3.6/site-packages',\n",
       " '/home/xuenhui/anaconda3/envs/patch_base-py362/lib/python3.6/site-packages/IPython/extensions',\n",
       " '/home/xuenhui/.ipython']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c41e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#添加导包路径\n",
    "sys.path.append('/home/xuenhui/anaconda3/envs/patch-py362/lib/python36.zip')\n",
    "sys.path.append('/home/xuenhui/anaconda3/envs/patch-py362/lib/python3.6')\n",
    "sys.path.append('/home/xuenhui/anaconda3/envs/patch-py362/lib/python3.6/lib-dynload')\n",
    "sys.path.append('/home/xuenhui/anaconda3/envs/patch-py362/lib/python3.6/site-packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c310681b",
   "metadata": {},
   "source": [
    "This solution uses only machine learning methods to \"blind guess\" from file name information.\n",
    "\n",
    "- Specifically, the name of the video file is extracted and divided into segments, and a total of 5 features are extracted, and after feature encoding, they are sent to XGB for training.\n",
    "\n",
    "- Best score on test1:\n",
    "\n",
    "  - result using original parameters: 0.708238；0.682046\n",
    "\n",
    "  - Results after using grid search: 0.7501；0.7179 (Final Results)\n",
    "  \n",
    "\n",
    "Subsequent optimization ideas:\n",
    "- [x] Feature stitching with fea340 or 480\n",
    "- [ ] Extract the training data of the same type as test1 in the training set (same forgery method), and train test1 separately\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4687ea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.read_csv('data/label/train_set.csv')\n",
    "test1 = pd.read_csv('data/label/test_set1.txt', names=['file'])\n",
    "test2 = pd.read_csv('data/label/test_set2.txt', names=['file'])\n",
    "test3 = pd.read_csv('data/label/test_set3.txt', names=['file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0730144",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train.drop(['mos'],axis = 1)\n",
    "# labels = train['mos']\n",
    "labels = np.array(list(train['mos']), dtype=np.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a866700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(labels))\n",
    "print(type(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a63bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "features['file_name'] = features['file'].apply(lambda x: x.split(\"/\")[0])\n",
    "features['id1'] = features['file'].apply(lambda x: x.split(\"/\")[1].split(\"-\")[0])\n",
    "features['id2'] = features['file'].apply(lambda x: x.split(\"/\")[1].split(\"-\")[1])\n",
    "features['id3'] = features['file'].apply(lambda x: x.split(\"/\")[1].split(\"-\")[2])\n",
    "features['man'] = features['file'].apply(lambda x: x.split(\"/\")[1].split(\"-\")[-1].split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "923b93cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_features = test1\n",
    "test1_features['file_name'] = test1_features['file'].apply(lambda x: x.split(\"/\")[0])\n",
    "test1_features['id1'] = test1_features['file'].apply(lambda x: x.split(\"/\")[1].split(\"-\")[0])\n",
    "test1_features['id2'] = test1_features['file'].apply(lambda x: x.split(\"/\")[1].split(\"-\")[1])\n",
    "test1_features['id3'] = test1_features['file'].apply(lambda x: x.split(\"/\")[1].split(\"-\")[2])\n",
    "test1_features['man'] = test1_features['file'].apply(lambda x: x.split(\"/\")[1].split(\"-\")[-1].split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efe57616",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_features = test2\n",
    "test2_features['file_name'] = test2_features['file'].apply(lambda x: x.split(\"/\")[0])\n",
    "test2_features['id1'] = test2_features['file'].apply(lambda x: x.split(\"/\")[1].split(\"-\")[0])\n",
    "test2_features['id2'] = test2_features['file'].apply(lambda x: x.split(\"/\")[1].split(\"-\")[1])\n",
    "test2_features['id3'] = test2_features['file'].apply(lambda x: x.split(\"/\")[1].split(\"-\")[2])\n",
    "test2_features['man'] = test2_features['file'].apply(lambda x: x.split(\"/\")[1].split(\"-\")[-1].split(\".\")[0])\n",
    "\n",
    "test3_features = test3\n",
    "test3_features['file_name'] = test3_features['file'].apply(lambda x: x.split(\"/\")[0])\n",
    "test3_features['id1'] = test3_features['file'].apply(lambda x: x.split(\"/\")[1].split(\"-\")[0])\n",
    "test3_features['id2'] = test3_features['file'].apply(lambda x: x.split(\"/\")[1].split(\"-\")[1])\n",
    "test3_features['id3'] = test3_features['file'].apply(lambda x: x.split(\"/\")[1].split(\"-\")[2])\n",
    "test3_features['man'] = test3_features['file'].apply(lambda x: x.split(\"/\")[1].split(\"-\")[-1].split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12df14de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a tag before merging to facilitate splitting\n",
    "features['tag'] = 'train'\n",
    "test1_features['tag'] = 'test1'\n",
    "test2_features['tag'] = 'test2'\n",
    "test3_features['tag'] = 'test3'\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# It is necessary to combine the training set and the test set, and then do LabelEncoder uniformly\n",
    "all_df = pd.concat([features, test1_features, test2_features, test3_features])\n",
    "all_df['file'] = all_df['file'].astype('str')\n",
    "all_df['file'] = le.fit_transform(all_df['file'])\n",
    "# Perform LabelEncoder on id1\n",
    "all_df['id1'] = all_df['id1'].astype('str')\n",
    "all_df['id1'] = le.fit_transform(all_df['id1'])\n",
    "\n",
    "all_df['id2'] = all_df['id2'].astype('str')\n",
    "all_df['id2'] = le.fit_transform(all_df['id2'])\n",
    "\n",
    "all_df['id3'] = all_df['id3'].astype('str')\n",
    "all_df['id3'] = le.fit_transform(all_df['id3'])\n",
    "\n",
    "all_df['file_name'] = all_df['file_name'].astype('str')\n",
    "all_df['file_name'] = le.fit_transform(all_df['file_name'])\n",
    "\n",
    "all_df['man'] = all_df['man'].astype('str')\n",
    "all_df['man'] = le.fit_transform(all_df['man'])\n",
    "\n",
    "\n",
    "\n",
    "# Split the encoded features into train, test1, test2, test3\n",
    "features['file'] = all_df[all_df['tag']=='train']['file']\n",
    "features['file_name'] = all_df[all_df['tag']=='train']['file_name']\n",
    "features['id1'] = all_df[all_df['tag']=='train']['id1']\n",
    "features['id2'] = all_df[all_df['tag']=='train']['id2']\n",
    "features['id3'] = all_df[all_df['tag']=='train']['id3']\n",
    "features['man'] = all_df[all_df['tag']=='train']['man']\n",
    "features = features.drop(['tag'],axis = 1)\n",
    "features = features.drop(['file'],axis = 1)\n",
    "features\n",
    "\n",
    "\n",
    "test1_features['file'] = all_df[all_df['tag']=='test1']['file']\n",
    "test1_features['file_name'] = all_df[all_df['tag']=='test1']['file_name']\n",
    "test1_features['id1'] = all_df[all_df['tag']=='test1']['id1']\n",
    "test1_features['id2'] = all_df[all_df['tag']=='test1']['id2']\n",
    "test1_features['id3'] = all_df[all_df['tag']=='test1']['id3']\n",
    "test1_features['man'] = all_df[all_df['tag']=='test1']['man']\n",
    "test1_features = test1_features.drop(['tag'],axis = 1)\n",
    "test1_features = test1_features.drop(['file'],axis = 1)\n",
    "\n",
    "\n",
    "test2_features['file'] = all_df[all_df['tag']=='test2']['file']\n",
    "test2_features['file_name'] = all_df[all_df['tag']=='test2']['file_name']\n",
    "test2_features['id1'] = all_df[all_df['tag']=='test2']['id1']\n",
    "test2_features['id2'] = all_df[all_df['tag']=='test2']['id2']\n",
    "test2_features['id3'] = all_df[all_df['tag']=='test2']['id3']\n",
    "test2_features['man'] = all_df[all_df['tag']=='test2']['man']\n",
    "test2_features = test2_features.drop(['tag'],axis = 1)\n",
    "test2_features = test2_features.drop(['file'],axis = 1)\n",
    "\n",
    "\n",
    "test3_features['file'] = all_df[all_df['tag']=='test3']['file']\n",
    "test3_features['file_name'] = all_df[all_df['tag']=='test3']['file_name']\n",
    "test3_features['id1'] = all_df[all_df['tag']=='test3']['id1']\n",
    "test3_features['id2'] = all_df[all_df['tag']=='test3']['id2']\n",
    "test3_features['id3'] = all_df[all_df['tag']=='test3']['id3']\n",
    "test3_features['man'] = all_df[all_df['tag']=='test3']['man']\n",
    "test3_features = test3_features.drop(['tag'],axis = 1)\n",
    "test3_features = test3_features.drop(['file'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "482700ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[ 0  0  0  7  0]\n",
      " [ 0  0  0  7  2]\n",
      " [ 0  0  0  7  3]\n",
      " ...\n",
      " [ 2 19  1 12 30]\n",
      " [ 2 19  1 12 31]\n",
      " [ 2 19  1 12 32]]\n"
     ]
    }
   ],
   "source": [
    "features = features.values\n",
    "print(type(features))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68a26529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7386349841110025"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=1)\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=200, max_depth=6,\n",
    "         subsample=0.6, colsample_bytree=0.8, learning_rate=0.05, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15db8e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4170655737262433"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "pred = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "817b7d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在3折数据上的交叉验证\n",
      "均方根误差:\n",
      "0.4404700223612908\n",
      "拟合优度\n",
      "0.6704943023231531\n",
      "均方根误差:\n",
      "0.4058233239969286\n",
      "拟合优度\n",
      "0.7354703916906666\n",
      "均方根误差:\n",
      "0.429908598706019\n",
      "拟合优度\n",
      "0.6903203544956465\n",
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n",
      "0.7091687456866139\n",
      "{'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "# regression cross validation\n",
    "rng = np.random.RandomState(123)\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=rng)\n",
    "print(\"在3折数据上的交叉验证\")\n",
    "\n",
    "for train_index, test_index in kf.split(features):\n",
    "    xgb_model = xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                                 n_estimators=300, \n",
    "                                 max_depth=6,\n",
    "                                 subsample=0.6,\n",
    "                                 colsample_bytree=0.8,learning_rate=0.1,random_state=0)\n",
    "    xgb_model.fit(features[train_index],labels[train_index])                             \n",
    "    predictions = xgb_model.predict(features[test_index])\n",
    "    actuals = labels[test_index]\n",
    "    print(\"均方根误差:\")\n",
    "    print(np.sqrt(mean_squared_error(actuals, predictions)))\n",
    "    print('拟合优度')\n",
    "    print(xgb_model.score(features[test_index],labels[test_index]))\n",
    "\n",
    "    \n",
    "    \n",
    "# Regression grid search for optimal hyperparameters\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                         subsample=0.6, \n",
    "                         colsample_bytree=0.8, \n",
    "                         random_state=0,nthread=8)\n",
    "param_dict = {'max_depth': [5,6,7,8],\n",
    "              'n_estimators': [200,150,250],\n",
    "              'learning_rate':[0.05,0.1,0.2]}\n",
    "              \n",
    "clf = GridSearchCV(model, param_dict, cv=10, verbose=1 , scoring='r2')\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_score_)\n",
    "print(clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f12b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "test1_pred = clf.predict(test1_features)\n",
    "test2_pred = clf.predict(test2_features)\n",
    "test3_pred = clf.predict(test3_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d09b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest1 = pd.read_csv('data/label/test_set1.txt', names=['file'])\n",
    "ttest2 = pd.read_csv('data/label/test_set2.txt', names=['file'])\n",
    "ttest3 = pd.read_csv('data/label/test_set3.txt', names=['file'])\n",
    "#save result\n",
    "import os\n",
    "test1_res = pd.DataFrame(ttest1['file'])\n",
    "test1_res['mos'] = test1_pred\n",
    "test1_res.to_csv(os.path.join(r'./pred_3(aistudio)/grid_search/', 'Test1_preds.txt'), index=None, header=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (patch_base-py362)",
   "language": "python",
   "name": "patch_base-py362"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
